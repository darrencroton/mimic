#!/usr/bin/env python3
"""
Scientific Validation Test (Metadata-Driven)

Validates: Physical correctness and reasonable ranges of ALL output properties
Phase: Phase 3 (Metadata-Driven Testing)

This comprehensive test dynamically validates ALL output properties based on YAML metadata:
- NaN/Inf Detection: Checks ALL floating-point properties (FAIL if found)
- Zero Value Warnings: Checks ALL numeric properties (WARNING only, respects sentinels)
- Range Validation: Checks properties with defined ranges in YAML (FAIL if violated)

Key Features:
- Fully metadata-driven: validation rules come from property YAML files
- No hardcoded property lists: automatically adapts to property changes
- Sentinel awareness: respects sentinel values (e.g., 0.0, -1.0) in validation

Validation Rules Source:
- Generated from: metadata/*.yaml
- Manifest file: tests/generated/property_ranges.json
- Regenerate: make generate

Note: Internal units are 10^10 Msun/h for masses

Author: Mimic Testing Team
Date: 2025-11-11 (Updated for metadata-driven validation)
"""

import subprocess
import sys
from pathlib import Path
import numpy as np
import json

# Add framework to path
REPO_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(REPO_ROOT / "tests"))

from framework import load_binary_halos

# Repository paths
TEST_DATA_DIR = REPO_ROOT / "tests" / "data"
MIMIC_EXE = REPO_ROOT / "mimic"


def ensure_output_dirs():
    """
    Create output directories if they don't exist

    Creates the binary and HDF5 output directories required by test parameter files.
    This ensures tests work correctly after make test-clean or in fresh clones.
    """
    (TEST_DATA_DIR / "output" / "binary").mkdir(parents=True, exist_ok=True)
    (TEST_DATA_DIR / "output" / "hdf5").mkdir(parents=True, exist_ok=True)


# Ensure output directories exist before any tests run
ensure_output_dirs()

# Validation manifest (auto-generated by scripts/generate_properties.py)
VALIDATION_MANIFEST_PATH = REPO_ROOT / "tests" / "generated" / "property_ranges.json"

# ANSI color codes
RED = '\033[0;31m'
GREEN = '\033[0;32m'
YELLOW = '\033[1;33m'
NC = '\033[0m'  # No Color


def run_mimic_if_needed():
    """
    Run Mimic if output doesn't exist

    Returns:
        Path: Path to output file
    """
    output_dir = TEST_DATA_DIR / "output" / "binary"
    output_file = output_dir / "model_z0.000_0"  # snapshot 63 is z=0

    if not output_file.exists():
        print("  Running Mimic to generate output...")
        param_file = TEST_DATA_DIR / "test_binary.par"
        result = subprocess.run(
            [str(MIMIC_EXE), str(param_file)],
            cwd=str(REPO_ROOT),
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            print(f"STDOUT:\n{result.stdout}")
            print(f"STDERR:\n{result.stderr}")
            raise RuntimeError(f"Mimic execution failed with code {result.returncode}")

    return output_file


def check_nans_infs(halos):
    """
    Check for NaN and Inf values in ALL output fields dynamically

    Returns:
        dict: Results with 'passed', 'nan_fields', 'inf_fields'
    """
    nan_fields = {}
    inf_fields = {}

    # Check all fields in the dtype
    for field in halos.dtype.names:
        data = getattr(halos, field)

        # Check for NaN (only for float types)
        if data.dtype.kind == 'f':  # floating point
            if data.ndim == 1:
                # Scalar field
                nan_count = np.sum(np.isnan(data))
                if nan_count > 0:
                    indices = np.where(np.isnan(data))[0][:5]
                    nan_fields[field] = {
                        'count': nan_count,
                        'examples': [(int(i), float(data[i])) for i in indices]
                    }

                inf_count = np.sum(np.isinf(data))
                if inf_count > 0:
                    indices = np.where(np.isinf(data))[0][:5]
                    inf_fields[field] = {
                        'count': inf_count,
                        'examples': [(int(i), float(data[i])) for i in indices]
                    }
            else:
                # Vector field
                nan_count = np.sum(np.isnan(data))
                if nan_count > 0:
                    nan_fields[field] = {'count': nan_count, 'examples': []}

                inf_count = np.sum(np.isinf(data))
                if inf_count > 0:
                    inf_fields[field] = {'count': inf_count, 'examples': []}

    return {
        'passed': len(nan_fields) == 0 and len(inf_fields) == 0,
        'nan_fields': nan_fields,
        'inf_fields': inf_fields
    }


def check_zeros(halos, manifest):
    """
    Check for zero values in ALL numeric output properties dynamically
    Suppress warnings for properties with sentinels that include 0 or 0.0
    These are warnings, not failures

    Returns:
        dict: Field name -> count and examples
    """
    zero_counts = {}
    props = manifest.get('properties', {})

    # Check all numeric fields in the dtype
    for field in halos.dtype.names:
        data = getattr(halos, field)

        # Only check numeric types (not in vectors for now, to keep output manageable)
        if data.ndim == 1 and data.dtype.kind in ('f', 'i'):  # float or int
            # Check if this property has 0/0.0 in sentinels
            spec = props.get(field, {})
            sentinels = set(spec.get('sentinels', []))

            # Skip zero check if 0 or 0.0 is in sentinels (intentional)
            if 0 in sentinels or 0.0 in sentinels:
                continue

            # Check for zeros
            if data.dtype.kind == 'f':
                zero_mask = data == 0.0
            else:
                zero_mask = data == 0

            count = np.sum(zero_mask)
            if count > 0:
                indices = np.where(zero_mask)[0][:5]
                zero_counts[field] = {
                    'count': count,
                    'examples': [(int(i), float(data[i]) if data.dtype.kind == 'f' else int(data[i])) for i in indices]
                }

    return zero_counts


def check_range(halos, field, min_val, max_val, exclude_zeros=False):
    """
    Check if field values are within expected range

    Args:
        halos: Halo data
        field: Field name
        min_val: Minimum expected value
        max_val: Maximum expected value
        exclude_zeros: If True, don't count zeros as below minimum

    Returns:
        dict: Results with 'passed', counts, examples
    """
    data = getattr(halos, field)

    if exclude_zeros:
        # For range checking, exclude zeros (they're already warned about)
        valid_mask = data != 0.0
        data_to_check = data[valid_mask]
    else:
        data_to_check = data

    below_min = (data_to_check < min_val)
    above_max = (data_to_check > max_val)

    count_below = np.sum(below_min)
    count_above = np.sum(above_max)

    result = {
        'passed': count_below == 0 and count_above == 0,
        'min_value': float(np.min(data_to_check)) if len(data_to_check) > 0 else 0.0,
        'max_value': float(np.max(data_to_check)) if len(data_to_check) > 0 else 0.0,
        'count_below': count_below,
        'count_above': count_above,
        'examples_below': [],
        'examples_above': []
    }

    if count_below > 0:
        if exclude_zeros:
            # Find original indices
            original_indices = np.where(valid_mask)[0]
            below_in_valid = np.where(below_min)[0][:5]
            actual_indices = original_indices[below_in_valid]
        else:
            actual_indices = np.where(below_min)[0][:5]

        result['examples_below'] = [(int(i), float(data[i])) for i in actual_indices]

    if count_above > 0:
        if exclude_zeros:
            original_indices = np.where(valid_mask)[0]
            above_in_valid = np.where(above_max)[0][:5]
            actual_indices = original_indices[above_in_valid]
        else:
            actual_indices = np.where(above_max)[0][:5]

        result['examples_above'] = [(int(i), float(data[i])) for i in actual_indices]

    return result


def test_numerical_validity():
    """
    Test for NaN and Inf values (critical failures)
    """
    print()
    print("="*60)
    print("NUMERICAL VALIDITY (NaN/Inf checks)")
    print("="*60)

    if not MIMIC_EXE.exists():
        print(f"  Skipping (Mimic not built)")
        return True, 0

    output_file = run_mimic_if_needed()
    halos, metadata = load_binary_halos(output_file)
    print(f"Loaded {metadata['TotHalos']} halos from {metadata['Ntrees']} trees\n")

    result = check_nans_infs(halos)

    if result['nan_fields']:
        print(f"{RED}✗ FAIL: Found NaN values in {len(result['nan_fields'])} field(s):{NC}")
        for field, info in result['nan_fields'].items():
            print(f"  {field}: {info['count']} NaN values")
            if info['examples']:
                for idx, val in info['examples']:
                    print(f"    Halo {idx}: {field} = {val}")
        return False, 1

    if result['inf_fields']:
        print(f"{RED}✗ FAIL: Found Inf values in {len(result['inf_fields'])} field(s):{NC}")
        for field, info in result['inf_fields'].items():
            print(f"  {field}: {info['count']} Inf values")
            if info['examples']:
                for idx, val in info['examples']:
                    print(f"    Halo {idx}: {field} = {val}")
        return False, 1

    print(f"{GREEN}✓ PASS: No NaN or Inf values found{NC}")
    return True, 0


def test_zero_values():
    """
    Check for zero values dynamically for ALL properties (warnings, not failures)
    Respects sentinels - suppresses warnings for properties where 0/0.0 is intentional
    """
    print()
    print("="*60)
    print("ZERO VALUE CHECKS (warnings, respects sentinels)")
    print("="*60)

    # Load validation manifest
    if not VALIDATION_MANIFEST_PATH.exists():
        print(f"{YELLOW}⚠ WARNING: Validation manifest not found: {VALIDATION_MANIFEST_PATH}{NC}")
        print("  Run 'make generate' to create property_ranges.json from YAML metadata.")
        print("  Skipping zero-value checks.")
        return True, 0

    try:
        with open(VALIDATION_MANIFEST_PATH) as f:
            manifest = json.load(f)
    except Exception as e:
        print(f"{RED}✗ FAIL: Could not parse validation manifest: {e}{NC}")
        return False, 1

    if not MIMIC_EXE.exists():
        print(f"  Skipping (Mimic not built)")
        return True, 0

    output_file = run_mimic_if_needed()
    halos, metadata = load_binary_halos(output_file)
    total_halos = metadata['TotHalos']

    zero_counts = check_zeros(halos, manifest)

    if zero_counts:
        print(f"{YELLOW}⚠ WARNING: Found zero values in {len(zero_counts)} field(s):{NC}")
        print(f"(Properties with sentinel 0/0.0 are not shown)")
        for field, info in zero_counts.items():
            print(f"  {field}: {info['count']} zero values out of {total_halos} total")
            for idx, val in info['examples'][:3]:  # Limit to 3 examples
                print(f"    Halo {idx}: {field} = {val}")
        return True, len(zero_counts)  # Warnings, not failures
    else:
        print(f"{GREEN}✓ No unexpected zero values found{NC}")
        return True, 0


def test_physical_ranges():
    """
    Test that all OUTPUT properties are within ranges defined in metadata
    """
    print()
    print("="*60)
    print("PHYSICAL RANGE VALIDATION")
    print("="*60)

    # Load validation manifest
    if not VALIDATION_MANIFEST_PATH.exists():
        print(f"{YELLOW}⚠ WARNING: Validation manifest not found: {VALIDATION_MANIFEST_PATH}{NC}")
        print("  Run 'make generate' to create property_ranges.json from YAML metadata.")
        print("  Skipping range validation.")
        return True, 0

    try:
        with open(VALIDATION_MANIFEST_PATH) as f:
            manifest = json.load(f)
    except Exception as e:
        print(f"{RED}✗ FAIL: Could not parse validation manifest: {e}{NC}")
        return False, 1

    if not MIMIC_EXE.exists():
        print(f"  Skipping (Mimic not built)")
        return True, 0

    output_file = run_mimic_if_needed()
    halos, metadata = load_binary_halos(output_file)

    failures = 0

    props = manifest.get('properties', {})
    output_fields = list(halos.dtype.names)

    # Report summary
    with_ranges = [k for k, v in props.items() if 'range' in v]
    print(f"Found {len(output_fields)} output fields in binary file.")
    print(f"Validation specs present for {len(with_ranges)} properties with ranges.\n")

    # Validate each field present in output and manifest
    for field in output_fields:
        spec = props.get(field)
        if not spec or 'range' not in spec:
            # No spec, skip with notice
            print(f"{YELLOW}Skipping {field}: no range specified in metadata{NC}")
            continue

        rmin, rmax = spec['range']
        sentinels = set(spec.get('sentinels', []))
        data = getattr(halos, field)

        # Build mask for values to check (exclude sentinels)
        if data.ndim == 1:
            mask = np.ones_like(data, dtype=bool)
            if len(sentinels) > 0:
                for s in sentinels:
                    mask &= (data != s)

            values = data[mask]

            below = np.sum(values < rmin)
            above = np.sum(values > rmax)
            total_checked = len(values)
            total_failed = below + above

            if total_failed > 0:
                failures += 1
                print(f"{RED}✗ FAIL: {field} outside [{rmin}, {rmax}] inclusive{NC}")
                print(f"  Actual range: {np.min(values):.4g} to {np.max(values):.4g}")
                print(f"  Failed: {total_failed} out of {total_checked} halos ({100.0*total_failed/total_checked:.1f}%)")
            else:
                print(f"{GREEN}✓ PASS: {field} within [{rmin}, {rmax}] (inclusive){NC}")

        else:
            # Vector field: check each component against same range
            # Note: sentinel handling for vectors is not applied unless exact scalar sentinels match per-component
            comp_fail = 0
            all_comp_values = []  # Track all component values for min/max reporting

            for ci in range(data.shape[1]):
                comp = data[:, ci]
                mask = np.ones_like(comp, dtype=bool)
                if len(sentinels) > 0:
                    for s in sentinels:
                        mask &= (comp != s)
                values = comp[mask]
                all_comp_values.extend(values)  # Collect values from all components

                below = np.sum(values < rmin)
                above = np.sum(values > rmax)
                if below > 0 or above > 0:
                    comp_fail += 1

            if comp_fail > 0:
                failures += 1
                all_vals = np.array(all_comp_values)
                # Count how many values failed across all components
                total_failed = np.sum((all_vals < rmin) | (all_vals > rmax))
                total_checked = len(all_vals)

                print(f"{RED}✗ FAIL: {field} component(s) outside [{rmin}, {rmax}] inclusive{NC}")
                print(f"  Actual range: {np.min(all_vals):.4g} to {np.max(all_vals):.4g} (across all components)")
                print(f"  Failed: {total_failed} out of {total_checked} values ({100.0*total_failed/total_checked:.1f}%)")
            else:
                print(f"{GREEN}✓ PASS: {field} components within [{rmin}, {rmax}] (inclusive){NC}")

    print()
    return failures == 0, failures


def main():
    """
    Main test runner
    """
    print("Scientific Validation Test")
    print(f"Repository root: {REPO_ROOT}")
    print(f"Mimic executable: {MIMIC_EXE}")

    # Check prerequisites
    if not MIMIC_EXE.exists():
        print(f"{RED}ERROR: Mimic executable not found: {MIMIC_EXE}{NC}")
        print("Build it first with: make")
        return 1

    # Run test sections
    passed_sections = 0
    failed_sections = 0
    warning_count = 0

    # 1. Numerical validity (critical)
    passed, failures = test_numerical_validity()
    if passed:
        passed_sections += 1
    else:
        failed_sections += failures

    # 2. Zero values (warnings)
    passed, warnings = test_zero_values()
    warning_count = warnings
    if passed:
        passed_sections += 1

    # 3. Physical ranges
    passed, failures = test_physical_ranges()
    if passed:
        passed_sections += 1
    else:
        failed_sections += failures

    # Summary
    print("=" * 60)
    print("Test Summary: Scientific Validation")
    print("=" * 60)
    print(f"Sections passed: {passed_sections}")
    print(f"Sections failed: {failed_sections}")
    if warning_count > 0:
        print(f"{YELLOW}Warnings: {warning_count} field(s) with zero values{NC}")
    print("=" * 60)

    if failed_sections == 0:
        if warning_count > 0:
            print(f"{YELLOW}✓ All tests passed (with warnings){NC}")
        else:
            print(f"{GREEN}✓ All tests passed!{NC}")
        return 0
    else:
        print(f"{RED}✗ {failed_sections} test(s) failed{NC}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
